{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16f95c63-89cc-49dd-b6dd-7883ecdc8005",
   "metadata": {},
   "source": [
    "import inspect # view the source code using ---> getsourcelines(thing)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257d3402-22d1-4f71-9063-27604db3928b",
   "metadata": {},
   "source": [
    "print(f\"PyTorch version: {torch.__version__}\\ntorchvision version: {torchvision.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4e0876-ffae-4005-af8d-35fecb8285a2",
   "metadata": {},
   "source": [
    "train_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    "    target_transform=None,\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    "    target_transform=None,\n",
    ")\n",
    "\n",
    "class_names = train_data.classes\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075dcfcf-a5bb-45bf-8177-8d5021d6decb",
   "metadata": {},
   "source": [
    "len(train_data.data), len(test_data.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11047704-2b5f-4ded-8f91-e99b7eb8a930",
   "metadata": {},
   "source": [
    "# iterator example\n",
    "\n",
    "for x in np.random.rand(5): print(x)\n",
    "\n",
    "print(), print()\n",
    "\n",
    "stuff = np.random.rand(5)\n",
    "print(stuff), print()\n",
    "it = iter(stuff)\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        item = next(it)\n",
    "        print(item)\n",
    "    except StopIteration:\n",
    "        print(\"StopIteration exception raised\")\n",
    "        break\n",
    "        \n",
    "\n",
    "stuff = np.random.rand(5)\n",
    "print(), print(), print(stuff), print()\n",
    "it = iter(stuff)\n",
    "\n",
    "# this will raise exception\n",
    "#while True:\n",
    "#    item = next(it)\n",
    "#    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80eb4ef4-0f41-4510-b0fe-fe31abf2e11c",
   "metadata": {},
   "source": [
    "image, label = train_data[0]\n",
    "image = image.detach().numpy()\n",
    "plt.imshow(image.squeeze(), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19b9774-289e-4698-a78c-dcaf8adc8157",
   "metadata": {},
   "source": [
    "rows, cols = 4, 4\n",
    "fig = plt.figure(figsize=(6,6))\n",
    "for i in range(rows * cols):\n",
    "    image, label = train_data[i]\n",
    "    image = image.detach().numpy().squeeze()\n",
    "    fig.add_subplot(rows, cols, i+1)\n",
    "    plt.imshow(image, cmap=\"gray\")\n",
    "    plt.title(f\"[{i}] {train_data.classes[label]}\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31a41b3-1140-4908-83d3-6c41dd8e5e24",
   "metadata": {},
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Setup the batch size hyperparameter\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Turn datasets into iterables (batches)\n",
    "train_dataloader = DataLoader(train_data, # dataset to turn into iterable\n",
    "    batch_size=BATCH_SIZE, # how many samples per batch? \n",
    "    shuffle=True # shuffle data every epoch?\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(test_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False # don't necessarily have to shuffle the testing data\n",
    ")\n",
    "\n",
    "# Let's check out what we've created\n",
    "print(f\"Dataloaders: {train_dataloader, test_dataloader}\") \n",
    "print(f\"Length of train dataloader: {len(train_dataloader)} batches of {BATCH_SIZE}\")\n",
    "print(f\"Length of test dataloader: {len(test_dataloader)} batches of {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f61539-44d2-4ec0-b9a9-f8e5ead64818",
   "metadata": {},
   "source": [
    "a = iter(train_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0369ce-a199-4c52-a1cd-882a8bd6ea2b",
   "metadata": {},
   "source": [
    "b = next(a)\n",
    "\n",
    "print(f\"b[0] (batch of samples, length: {len(b[0])})\\nEach sized: {b[0][0].shape}\\n\")\n",
    "print(f\"b[1] (batch of labels, length: {len(b[1])}): \\n{b[1]}\\n\")\n",
    "\n",
    "# next(a)\n",
    "# [0] (e.g. train_features_batch)\n",
    "#    ---> array of tensors (X data),   length: batch size\n",
    "# [1] (e.g. train_labels_batch)\n",
    "#    ---> array of tensors (y labels), length: batch size\n",
    "\n",
    "# second index: current sample in current batch\n",
    "\n",
    "img = b[0][0].detach().numpy().squeeze()\n",
    "lab = b[1][0].detach().numpy()\n",
    "print(f\"b[1][0] (current label: {lab}), type: {type(lab)}\")\n",
    "\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.title(f\"{lab}, {train_data.classes[lab]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89b7c7f-e62d-43ea-b590-aa3fbe4b4fbd",
   "metadata": {},
   "source": [
    "train_features_batch, train_labels_batch = next(iter(train_dataloader))\n",
    "train_features_batch.shape, train_labels_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cc377f-b9cf-4604-8a6b-cecc13503863",
   "metadata": {},
   "source": [
    "flatten_model = nn.Flatten()\n",
    "\n",
    "x = train_features_batch[0]\n",
    "\n",
    "output = flatten_model(x)\n",
    "\n",
    "print(f\"shape before flatten: {x.shape}      -> [colour channels, height, width]\")\n",
    "print(f\"shape after flatten:  {output.shape} -> [colour channels, height*width]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd900e3-6b7b-4fe9-bc80-2fd045fc6a4c",
   "metadata": {},
   "source": [
    "class FashionMNISTModelV0(nn.Module):\n",
    "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
    "        super().__init__()\n",
    "        self.layer_stack = nn.Sequential(\n",
    "            nn.Flatten(), # will take 28x28 image and output 28*28 (784) vector\n",
    "            nn.Linear(in_features=input_shape, out_features=hidden_units), # will take 784 vector as input (input_shape arg)\n",
    "            nn.Linear(in_features=hidden_units, out_features=output_shape)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layer_stack(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c241c054-8a24-448d-abe8-950cbfe43df4",
   "metadata": {},
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "model_0 = FashionMNISTModelV0(input_shape=28*28, hidden_units=10, output_shape=len(class_names))\n",
    "model_0.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a54a0d4-493a-4b83-958c-08bbcbb39610",
   "metadata": {},
   "source": [
    "def accuracy_fn(y_pred, y_true):\n",
    "    #prediction_index = y_pred.detach().numpy().argmax(axis=1)\n",
    "    #true_index = y_true.detach().numpy()\n",
    "    # must be converted before the function\n",
    "    l = len(y_true)\n",
    "    return np.sum(y_pred==y_true)/l\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(params=model_0.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bff0905-e2f2-4285-8e18-3fde63b9199f",
   "metadata": {},
   "source": [
    "test_batch = next(iter(test_dataloader))\n",
    "test_batch_samples = test_batch[0]\n",
    "test_batch_labels = test_batch[1]\n",
    "\n",
    "\n",
    "print(len(test_batch))\n",
    "\n",
    "model_0.eval()\n",
    "with torch.inference_mode():\n",
    "    \n",
    "    preds = model_0(test_batch_samples)\n",
    "\n",
    "    print(preds.shape)\n",
    "    \n",
    "    loss = loss_fn(preds, test_batch_labels)\n",
    "    print(loss.item())\n",
    "    \n",
    "    acc = accuracy_fn(y_pred=preds.detach().numpy().argmax(axis=1), y_true=test_batch_labels.detach().numpy())\n",
    "    print(acc)\n",
    "    \n",
    "preds_np = preds.detach().numpy().argmax(axis=1)\n",
    "trues_np = test_batch_labels.detach().numpy()\n",
    "\n",
    "print(preds_np.shape, trues_np.shape)\n",
    "    \n",
    "print(f\"predictions: {preds_np}\")\n",
    "\n",
    "print(f\"true vals  : {trues_np}\")\n",
    "\n",
    "print(f\"accuracy   : {accuracy_fn(y_pred=preds_np, y_true=trues_np)}\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "print(),print(),print()\n",
    "preds = preds[0].detach().numpy().argmax()\n",
    "print(preds)\n",
    "labels = test_batch_labels[0].detach().numpy()\n",
    "print(labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea14856-2209-4915-918a-ffde85849483",
   "metadata": {},
   "source": [
    "epochs = 3\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print(f\"Epoch: {epoch}\\n----------\")\n",
    "    \n",
    "    train_loss = 0\n",
    "    \n",
    "    # n_batch is the current batch\n",
    "    # (X, y) since train_dataloader has 2 items - data and labels, both are tensor arrays\n",
    "    for n_batch, (X, y) in enumerate(train_dataloader):\n",
    "        \n",
    "        y_pred = model_0(X)\n",
    "        \n",
    "        loss = loss_fn(y_pred, y)\n",
    "        \n",
    "        train_loss += loss # accumulate loss for each epoch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        if n_batch % 400 == 0:\n",
    "            print(f\"Looked at {n_batch * len(X)}/{len(train_dataloader.dataset)} samples\")\n",
    "            \n",
    "    train_loss /= len(train_dataloader) # get train loss percentage\n",
    "    \n",
    "    losses, accuracies = [], []\n",
    "    test_loss, test_acc = 0, 0\n",
    "    model_0.eval()\n",
    "    with torch.inference_mode():\n",
    "        for X, y in test_dataloader:\n",
    "            test_pred = model_0(X)\n",
    "            \n",
    "            loss = loss_fn(test_pred, y)\n",
    "            \n",
    "            test_loss += loss\n",
    "            \n",
    "            test_acc += accuracy_fn(y_pred=test_pred.detach().numpy().argmax(axis=1), y_true=y.detach().numpy())\n",
    "            #accuracies.append(test_acc)\n",
    "            a = test_acc/len(test_dataloader)*100\n",
    "            #print(a)\n",
    "            accuracies.append(a)\n",
    "        \n",
    "            \n",
    "        test_loss /= len(test_dataloader)\n",
    "        test_acc /= len(test_dataloader)\n",
    "        #accuracies.append(test_acc*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b3db51-0a28-4234-a0d1-14cf3598923b",
   "metadata": {},
   "source": [
    "plt.plot(accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddc8d3a-c53f-4d45-a723-3c459e8ec745",
   "metadata": {},
   "source": [
    "print(test_acc*100) # accuracy percentage\n",
    "print(test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726959a2-057b-4f2c-87a1-296fea1279b4",
   "metadata": {},
   "source": [
    "def eval_model(model: torch.nn.Module,\n",
    "                data_loader: torch.utils.data.DataLoader, # samples & labels batches for testing\n",
    "                loss_fn: torch.nn.Module,\n",
    "                accuracy_fn,\n",
    "                should_plot: bool):\n",
    "    \n",
    "    losses, accuracies = [], []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for X, y in data_loader:\n",
    "            \n",
    "            predictions = model(X)\n",
    "            \n",
    "            loss = loss_fn(predictions, y)\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            acc = accuracy_fn(predictions.detach().numpy().argmax(axis=1), y.detach().numpy())*100\n",
    "            accuracies.append(acc)           \n",
    "    \n",
    "    last_loss = losses[len(losses)-1]\n",
    "    last_acc = accuracies[len(accuracies)-1]\n",
    "    \n",
    "    if should_plot:\n",
    "        plt.plot(accuracies, c='b')\n",
    "        plt.plot([*range(len(accuracies))], [np.mean(accuracies) for _ in range(len(accuracies))], c=\"r\")\n",
    "        plt.plot(np.exp(losses), c='g')\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"Loss: {last_loss}\")\n",
    "        print(f\"Accuracy: {last_acc}\")\n",
    "    \n",
    "    return {\"model_name\": model.__class__.__name__,\n",
    "            \"model_loss\": last_loss,\n",
    "            \"model_accuracy\": last_acc}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6983fd-b792-41b3-8c10-9acb735130f1",
   "metadata": {},
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "model_0_results = eval_model(model_0, test_dataloader, loss_fn, accuracy_fn, True)\n",
    "print(model_0_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d549d7d-d89a-4f01-936e-b819e378854f",
   "metadata": {},
   "source": [
    "class FashionMNISTModelV1(nn.Module):\n",
    "    def __init__(self, input_shape, hidden_units, output_shape):\n",
    "        super().__init__()\n",
    "        self.layer_stack = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=input_shape, out_features=hidden_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=hidden_units, out_features=hidden_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=hidden_units, out_features=output_shape),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layer_stack(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd348f8c-0d8b-488c-8040-3e83e468a6ef",
   "metadata": {},
   "source": [
    "def train_model(model: nn.Module,\n",
    "                loss_fn,\n",
    "                optimizer, \n",
    "                dataloader: torch.utils.data.DataLoader,\n",
    "                n_epochs: int,\n",
    "                batch_size: int):\n",
    "  \n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        \n",
    "        print(f\"Epoch: {epoch}/{n_epochs}---------------\")\n",
    "    \n",
    "        for batch_n, (X, y) in enumerate(dataloader):\n",
    "            \n",
    "            if batch_n % 500 == 0: print(f\"Batch: {batch_n}/{len(dataloader)}---------------\")\n",
    "\n",
    "            y_predictions = model(X)\n",
    "            \n",
    "            loss = loss_fn(y_predictions, y)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            acc = accuracy_fn(y_predictions.detach().numpy().argmax(axis=1), y.detach().numpy())*100\n",
    "            accuracies.append(acc)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "                \n",
    "    plt.plot(accuracies)\n",
    "    plt.plot(np.exp(losses))\n",
    "    plt.ylim(0, 100)\n",
    "    plt.show()\n",
    "    print(f\"Accuracy: {accuracies[len(accuracies)-1]}\")\n",
    "    print(f\"Loss: {losses[len(losses)-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa516e7-dd0f-4eb1-9704-6b19661449ea",
   "metadata": {},
   "source": [
    "model_1 = FashionMNISTModelV1(28*28, 10, len(class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69ccb5d-6317-428f-bfc6-23369c69016a",
   "metadata": {},
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(params=model_1.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cc15b6-5301-4950-97be-840121b45eec",
   "metadata": {},
   "source": [
    "train_model(model_1, loss_fn, optimizer, train_dataloader, 3, 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb33d8c-1491-467c-a4a7-39265626baf4",
   "metadata": {},
   "source": [
    "model_1_results = eval_model(model_1, test_dataloader, loss_fn, accuracy_fn, True)\n",
    "model_1_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f727023-3e85-4df0-adda-dcf20b551af4",
   "metadata": {},
   "source": [
    "model_0_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4d1d9dd-f070-4713-8c67-7100f510c615",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mFashionMNISTModelV2\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_shape: \u001b[38;5;28mint\u001b[39m, hidden_units: \u001b[38;5;28mint\u001b[39m, output_shape: \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class FashionMNISTModelV2(nn.Module):\n",
    "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.block_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_shape,\n",
    "                      out_channels=hidden_units,\n",
    "                      kernel_size=3, # how big is the window going over the image\n",
    "                      stride=1,      # default\n",
    "                      padding=1),    # # options = \"valid\" (no padding) or \"same\" (output has same shape as input) or int for specific number \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=hidden_units,\n",
    "                      out_channels=hidden_units,\n",
    "                      kernel_size=3,\n",
    "                      stride=1,\n",
    "                      padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2,\n",
    "                         stride=2)\n",
    "        )\n",
    "        \n",
    "        self.block_2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=hidden_units, \n",
    "                      out_channels=hidden_units,\n",
    "                      kernel_size=3,\n",
    "                      padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=hidden_units,\n",
    "                      out_channels=hidden_units,\n",
    "                      kernel_size=3,\n",
    "                      padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=hidden_units*7*7,\n",
    "                      out_features=output_shape)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.block_1(x) #x.shape = torch.Size([32, 10, 14, 14])\n",
    "        x = self.block_2(x) #x.shape = torch.Size([32, 10, 7, 7])\n",
    "        x = self.classifier(x) #x.shape = torch.Size([32, 10])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aadf530-abde-45ee-ab04-4c2d16e77670",
   "metadata": {},
   "source": [
    "model_2 = FashionMNISTModelV2(input_shape=1,\n",
    "                              hidden_units=10,\n",
    "                              output_shape=len(class_names))\n",
    "model_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e685099-6823-478d-bcfb-0e7eef8c6ead",
   "metadata": {},
   "source": [
    "test_batch = next(iter(test_dataloader))\n",
    "test_batch_samples = test_batch[0]\n",
    "test_batch_labels = test_batch[1]\n",
    "\n",
    "print(len(test_batch))\n",
    "\n",
    "model_2.eval()\n",
    "with torch.inference_mode():\n",
    "    \n",
    "    preds = model_2(test_batch_samples)\n",
    "    \n",
    "    preds_np = preds.detach().numpy()\n",
    "    print(f\"Test predictions: {preds_np.argmax(axis=1)}\")  \n",
    "    print(f\"Test labels     : {test_batch_labels.detach().numpy()}\")\n",
    "    \n",
    "    loss = loss_fn(preds, test_batch_labels)\n",
    "    print(f\"Loss: {loss.item()}\")\n",
    "    \n",
    "    acc = accuracy_fn(y_pred=preds_np.argmax(axis=1),y_true=test_batch_labels.detach().numpy())*100\n",
    "    print(f\"Accuracy: {acc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24db17c-b896-4d75-b95b-b1423013307b",
   "metadata": {},
   "source": [
    "eval_model(model_2, test_dataloader, loss_fn, accuracy_fn, True)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fd8777-bd1a-4465-9373-2c7ebaccffa2",
   "metadata": {},
   "source": [
    "# getting a better understanding of dimensionality\n",
    "# represented in numpy arrays\n",
    "\n",
    "x = np.arange(200, dtype=np.float32)\n",
    "print(f\"Numpy arange: {x}\")\n",
    "print(f\"Numpy arange (16) shape: {x.shape}\\n\\n\")\n",
    "\n",
    "x = x.reshape(2, 4, 5, 5)  # bs, channels, height, width\n",
    "print(f\"Arange size 16 after reshape:\\n{x}\")\n",
    "print(f\"Numpy arange after reshape: {x.shape}\\n\\n\")\n",
    "\n",
    "print(), print()\n",
    "\n",
    "# x[0] is the \"first\" group of matrices\n",
    "for g in range(2):\n",
    "    for a, i in enumerate(x[g]):\n",
    "        print(a, i[2][2], i.shape)\n",
    "\n",
    "# intuitively: reshape(a, b, c, d)\n",
    "# a = number of groups of matrices\n",
    "# b = number of matrices in each group\n",
    "# x size of each matrix\n",
    "# y size of each matrix\n",
    "\n",
    "# above example:\n",
    "# 1 group of 4 matrices, sized 5x5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b35f975-2466-45b5-9eb5-d51f149b343c",
   "metadata": {},
   "source": [
    "x = np.arange(32, dtype=np.float32)\n",
    "print(f\"Numpy arange: {x}\")\n",
    "print(f\"Numpy arange (16) shape: {x.shape}\\n\\n\")\n",
    "\n",
    "x = x.reshape(1, 2, 4, 4)  # bs, channels, height, width\n",
    "print(f\"Arange size 16 after reshape:\\n{x}\")\n",
    "print(f\"Numpy arange after reshape: {x.shape}\\n\")\n",
    "\n",
    "X = torch.tensor(x, dtype=torch.float32).to(\"cpu\")\n",
    "print(\"\\nSource input: \")\n",
    "print(X)\n",
    "\n",
    "pool1 = nn.MaxPool2d(2, stride=1)\n",
    "z1 = pool1(X)\n",
    "print(\"\\nMaxPool with kernel=2, stride=1: \")\n",
    "print(z1)\n",
    "\n",
    "pool2 = nn.MaxPool2d(2, stride=2)\n",
    "z2 = pool2(X)\n",
    "print(\"\\nMaxPool with kernel=2, stride=2: \")\n",
    "print(z2)\n",
    "\n",
    "print(\"\\nEnd max pooling demo \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e825af-4782-4465-9d75-7ffb09079863",
   "metadata": {},
   "source": [
    "images = torch.randn(size=(32, 3, 64, 64)) # [batch_size, color_channels, height, width]\n",
    "test_image = images[0] # first \"group\" of 3 channels (colours) of 64x64 pixels\n",
    "\n",
    "conv_layer = nn.Conv2d(in_channels=3,\n",
    "                       out_channels=10,\n",
    "                       kernel_size=3,\n",
    "                       stride=1,\n",
    "                       padding=0)\n",
    "\n",
    "# unsqueeze(dim=0) adds an extra dimension\n",
    "print(test_image.shape) # original shape\n",
    "print(test_image.unsqueeze(dim=0).shape) # shape with extra dimension (its' own group)\n",
    "\n",
    "print(), print()\n",
    "\n",
    "print(test_image.shape) # original shape\n",
    "output = conv_layer(test_image.unsqueeze(dim=0)) # output of conv layer\n",
    "output.shape # output shape\n",
    "#print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573fe878-89c6-4441-8555-ce6caf020850",
   "metadata": {},
   "source": [
    "conv_layer_2 = nn.Conv2d(in_channels=3,\n",
    "                         out_channels=10,\n",
    "                         kernel_size=(2,2),\n",
    "                         stride=2.\n",
    "                         padding=0)\n",
    "\n",
    "\n",
    "conv_layer_2(test_image.unsqueeze(dim=0)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdc1d50-2a9e-45f9-876a-02ecdbf7af66",
   "metadata": {},
   "source": [
    "# Get shapes of weight and bias tensors within conv_layer_2\n",
    "print(f\"conv_layer_2 weight shape: \\n{conv_layer_2.weight.shape} -> [out_channels=10, in_channels=3, kernel_size=5, kernel_size=5]\")\n",
    "print(f\"\\nconv_layer_2 bias shape: \\n{conv_layer_2.bias.shape} -> [out_channels=10]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba64134-9560-4e8e-a07e-0426c2a791f9",
   "metadata": {},
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# getting a better understanding of the convolutional 2d layer in torch\n",
    "# example using a 2x2 matrix, with a single digit kernel\n",
    "\n",
    "a = np.arange(4).reshape(2,2)\n",
    "a = torch.from_numpy(a.astype(np.float32)).unsqueeze(dim=0).unsqueeze(dim=1)\n",
    "b = nn.Conv2d(in_channels=1,out_channels=1,kernel_size=(1),stride=1,padding=0)\n",
    "\n",
    "print(\"---- Original tensor ----\")\n",
    "print(a.shape), print(a), print()\n",
    "\n",
    "output = b(a)\n",
    "print(\"---- New tensor (OUTPUT) ----\")\n",
    "print(output.shape), \n",
    "print(output)\n",
    "\n",
    "print(),print()\n",
    "\n",
    "print(\"---- Conv2d layer ----\")\n",
    "print(\"Weight:\")\n",
    "print(b.weight.shape)\n",
    "print(b.weight)\n",
    "print()\n",
    "print(\"Bias:\")\n",
    "print(b.bias)\n",
    "\n",
    "print(),print()\n",
    "for i in range(2):\n",
    "    print(f\"index {i}: {a[0,0,0,i].detach().numpy()} * {b.weight.detach().numpy().squeeze()} + {b.bias.detach().numpy().squeeze()} = {(a[0,0,0,i] * b.weight + b.bias).detach().numpy().squeeze()}\")\n",
    "    \n",
    "for i in range(2):\n",
    "    print(f\"index {2 + i}: {a[0,0,1,i].detach().numpy()} * {b.weight.detach().numpy().squeeze()} + {b.bias.detach().numpy().squeeze()} = {(a[0,0,1,i] * b.weight + b.bias).detach().numpy().squeeze()}\")\n",
    "\n",
    "print(f\"compared to output: \\n{output.detach().numpy()}\")\n",
    "print(), print()\n",
    "\n",
    "\n",
    "print(\"Equals the same as the output\")\n",
    "print()\n",
    "print(\"Conv2d layer will multiply its' own randomly initialised weights\")\n",
    "#print(\"If the kernel size is bigger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035eddcb-e6c0-433a-91db-c7f26127b858",
   "metadata": {},
   "source": [
    "torch.manual_seed(42)\n",
    "print(\"Improving my understanding of the Conv2d layer in torch.\")\n",
    "print(\"This example demonstrates the calculation of the Conv2d layer,\\\n",
    "\\nwhich in this case has a kernel_size matching the input size.\")\n",
    "\n",
    "\n",
    "\n",
    "a = np.arange(4).reshape(2,2)\n",
    "a = torch.from_numpy(a.astype(np.float32)).unsqueeze(dim=0).unsqueeze(dim=1)\n",
    "a.requires_grad = False\n",
    "b = nn.Conv2d(in_channels=1,out_channels=1,kernel_size=((2,2)),stride=1,padding=0)\n",
    "print(f\"\\n{b}\\n\")\n",
    "\n",
    "print(\"---- Original tensor ----\")\n",
    "print(a.shape), print(a), print()\n",
    "\n",
    "output = b(a)\n",
    "print(\"---- New tensor (OUTPUT) ----\")\n",
    "print(output.shape), \n",
    "print(output)\n",
    "\n",
    "print(),print()\n",
    "\n",
    "print(\"---- Conv2d layer ----\")\n",
    "print(\"Weight:\")\n",
    "print(b.weight.shape)\n",
    "print(b.weight)\n",
    "print()\n",
    "print(\"Bias:\")\n",
    "print(b.bias)\n",
    "\n",
    "new_matrix = []\n",
    "\n",
    "print(),print()\n",
    "for i in range(2):\n",
    "    x = (a[0,0,0,i] * b.weight + b.bias).detach().numpy().squeeze()\n",
    "    print(f\"index {i}: {a[0,0,0,i].detach().numpy()} * {b.weight.detach().numpy().squeeze()} + {b.bias.detach().numpy().squeeze()} = {x}\")\n",
    "    new_matrix.append(x)\n",
    "for i in range(2):\n",
    "    x = (a[0,0,1,i] * b.weight + b.bias).detach().numpy().squeeze()\n",
    "    print(f\"index {2 + i}: {a[0,0,1,i].detach().numpy()} * {b.weight.detach().numpy().squeeze()} + {b.bias.detach().numpy().squeeze()} = {x}\")\n",
    "    new_matrix.append(x)\n",
    "    \n",
    "print()\n",
    "print(f\"compared to output: \\n{output.detach().numpy()} <------- figure out how it gets this output\")\n",
    "print(), print()\n",
    "\n",
    "print()\n",
    "print(\"Conv2d layer will multiply its' own randomly initialised weights\")\n",
    "#print(\"If the kernel size is bigger\n",
    "\n",
    "\n",
    "print(f\"output: {torch.sum(output).detach().numpy()}\")\n",
    "print(x, np.sum(x))\n",
    "\n",
    "\n",
    "print(np.multiply(b.weight.detach().numpy().squeeze().reshape(4), a.detach().numpy().squeeze().reshape(4)).sum() + b.bias.detach().numpy())\n",
    "\n",
    "print(b.weight.detach().numpy().squeeze().reshape(4).T.dot((a.squeeze().reshape(4))))\n",
    "\n",
    "s = 0\n",
    "for i in range(4): s += b.weight.detach().numpy().squeeze().reshape(4)[i] * a.detach().numpy().squeeze().reshape(4)[i]  \n",
    "print(str(s) + \"   + bias, this is correct\")\n",
    "\n",
    "print(np.sum(b.weight.detach().numpy().squeeze().dot(a.detach().numpy().squeeze())), end=\"\")\n",
    "print(\" <----- this is wrong\")\n",
    "\n",
    "print(\"\\n\\n\\\n",
    "Completely forgot about the bias, aside from the last 2x2.dot(2x2) dot product,\\nthe previous calculations are correct\\\n",
    " it seems to do an element-wise dot product,\\\n",
    " or it first flattens the input and then does a.T.dot(b) + bias,\\nwhich is equivalent to an element-wise dot product?\\\n",
    " \\n\\n\")\n",
    "\n",
    "\n",
    "print(\"Result of the final calculations which mimic the conv2d layer:\")\n",
    "\n",
    "\n",
    "# therefore when the kernel is the same size as the original image, and has a stride of 1,\n",
    "# the calculation is\n",
    "\n",
    "# dot product the flatten vectors \n",
    "result = b.weight.detach().numpy().squeeze().reshape(4).T.dot(a.detach().numpy().squeeze().reshape(4))\n",
    "# now add the bias to the result\n",
    "result += b.bias.detach().numpy()\n",
    "print(result, output.detach().numpy().squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4069f07d-22dc-443e-ab14-c6cdad01fa25",
   "metadata": {},
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# note:\n",
    "# the layer will have a tensor of weights with the SAME dimensions as the kernel size.\n",
    "##################\n",
    "\n",
    "# gain a better understanding of padding in the Conv2d layer.\n",
    "# few examples\n",
    "# padding will ADD pixels to the image, and default them as zero,\n",
    "# before running the calculation on the entire image, including the zeros.\n",
    "\n",
    "a = np.arange(9).reshape(3,3)\n",
    "a = torch.from_numpy(a.astype(np.float32)).unsqueeze(dim=0).unsqueeze(dim=1)\n",
    "b = nn.Conv2d(in_channels=1,out_channels=1,kernel_size=(1,1),stride=(1,1),padding=0) \n",
    "print(f\"b: {b}\"), print()\n",
    "print(f\"w: {b.weight}\"), print(f\"b: {b.bias}\")\n",
    "print(), print()\n",
    "\n",
    "print(\"----- without padding -----\")\n",
    "output = b(a)\n",
    "print(a), print()\n",
    "print(\"Output:\\n\", output), print()\n",
    "\n",
    "\n",
    "\n",
    "# copy the weight and bias, since the next example will have different value\n",
    "weight, bias = b.state_dict()[\"weight\"].data.detach(), b.state_dict()[\"bias\"].data.detach()\n",
    "print(f\"b parameters: {weight}, {bias}\")\n",
    "\n",
    "\n",
    "print(), print()\n",
    "\n",
    "\n",
    "print(\"----- with padding (1) -----\")\n",
    "b = nn.Conv2d(in_channels=1,out_channels=1,kernel_size=(3,3),stride=(1,1),padding=0)\n",
    "#b.weight.data = #weight\n",
    "b.weight.data = torch.ones((3,3)).unsqueeze(0).unsqueeze(1)\n",
    "b.bias.data = bias\n",
    "#b.bias.data = torch.zeros(1)\n",
    "print(f\"b: {b}\"), print()\n",
    "print(f\"w: {b.weight}\"), print(f\"b: {b.bias}\")\n",
    "print(), print()\n",
    "output = b(a)\n",
    "print(a), print()\n",
    "print(\"Output:\\n\", output)\n",
    "print(output.shape)\n",
    "\n",
    "print()\n",
    "print()\n",
    "print(\"The padding will add zeros to the outside of the image.\")\n",
    "print(\"The Conv2d layer will then perform the flattened.T dot product\")\n",
    "print(\"on the values on the outside, then add the bias\")\n",
    "print(\"In this case the outside values are 0.8300, matching the bias.\")\n",
    "print(\"If the bias is zero, the outside values will be zero after the calculations.\")\n",
    "print(\"\\n\\n\")\n",
    "print(\"Attempt to replicate kernel_size=(1,1), stride=(1,1), padding=(1)\")\n",
    "print(\"without the Conv2d layer:\")\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "a = a.detach().numpy().squeeze()\n",
    "print(\"a:\\n\", a), print()\n",
    "\n",
    "weights = torch.ones(3,3).unsqueeze(0).unsqueeze(1)\n",
    "bias = torch.Tensor([0.8300]) # hard coded values for testing\n",
    "print(weights, bias)\n",
    "\n",
    "a = a.flatten().T.dot(weights.squeeze().flatten()) + bias\n",
    "print(a, \"<----- matching the automated result above\")\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#padding\n",
    "#a = a.reshape((3,3))\n",
    "# add padding\n",
    "a = np.append(a, np.zeros((3,1)), axis=1)\n",
    "a = np.append(np.zeros((3,1)), a, axis=1)\n",
    "a = np.append(a, np.zeros((1,5)))\n",
    "a = np.append(np.zeros((1,5)), a)\n",
    "a = a.reshape(5,5)\n",
    "print(a)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b63f34-3147-4408-8646-ec9b7850ed2f",
   "metadata": {},
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# more padding examples and exploration\n",
    "# (for further understanding)\n",
    "\n",
    "\n",
    "data = torch.arange(36).type(torch.float32)\n",
    "data = data.reshape(6,6).unsqueeze(0).unsqueeze(1)\n",
    "print(data), print()\n",
    "\n",
    "KERNEL_SIZE = (2,2)\n",
    "STRIDE = (2,2)\n",
    "\n",
    "layer1 = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=KERNEL_SIZE, stride=STRIDE, padding=0)\n",
    "\n",
    "# get the first layer's weights\n",
    "weights, bias = layer1.weight.data, layer1.bias.data\n",
    "\n",
    "layer2 = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=KERNEL_SIZE, stride=STRIDE, padding=2)\n",
    "\n",
    "# copy it into the second layer's weights\n",
    "layer2.weight.data = weights\n",
    "layer2.bias.data = bias\n",
    "\n",
    "# check if the layers have matching weights\n",
    "print(torch.equal(layer1.weight.data, layer2.weight.data) and torch.equal(layer1.bias.data, layer2.bias.data)), print()\n",
    "\n",
    "result1 = layer1(data)\n",
    "print(\"result1\\n\", result1), print()\n",
    "\n",
    "addpad = nn.ReplicationPad2d(1)\n",
    "result1_pad = addpad(result1)\n",
    "print(\"result1pad\\n\", result1_pad), print()\n",
    "\n",
    "result2 = layer2(data)\n",
    "print(\"result2\\n\", result2), print()\n",
    "\n",
    "print(torch.eq(result1_pad, result2)), print()\n",
    "\n",
    "#nn.functional.pad(input=result1, pad=(1,1,1,1), value=0)\n",
    "\n",
    "\n",
    "#########\n",
    "# now with numpy\n",
    "print(\"###### now with numpy\\n\")\n",
    "\n",
    "data = nn.functional.pad(input=data, pad=(2,2,2,2), value=0)\n",
    "data = data.detach().numpy()\n",
    "weights = weights.detach().numpy()\n",
    "bias = bias.detach().numpy()\n",
    "\n",
    "\n",
    "data = data.squeeze()\n",
    "print(data),print()\n",
    "\n",
    "output = np.array([])\n",
    "\n",
    "for x in range(0, 10, 2):\n",
    "    for y in range(0, 10, 2):\n",
    "        cur2x2 = data[x:x+2,y:y+2]\n",
    "        res = cur2x2.flatten().T.dot(weights.flatten()) + bias\n",
    "        output = np.append(output, res.squeeze())    \n",
    "        \n",
    "output = output.reshape(5,5)  \n",
    "print(output), print()\n",
    "print(output == result2.detach().numpy().squeeze())\n",
    "\n",
    "print(\"\\nIt works and matches the previously calculated result2.\")\n",
    "print(\"Each row is flattened and transposed, then\")\n",
    "print(\"it is dot product'd with the weights (nxn)=kernel_size\")\n",
    "print(\"then the bias is added.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8c9d3b-33c6-44a0-a703-991bdb918ed7",
   "metadata": {},
   "source": [
    "# continuing with learnpytorch.io\n",
    "# 03: PyTorch Computer Vision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe7bcd5-445e-4961-ab4d-1f44adb23a8d",
   "metadata": {},
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(params=model_2.parameters(), lr=0.1)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "epochs = 3\n",
    "\n",
    "\n",
    "train_model(model_2, loss_fn, optimizer, train_dataloader, 3, 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abbb56d-0b42-437a-8e59-f5f3aa64dbfe",
   "metadata": {},
   "source": [
    "model_2_results = eval_model(model_2, test_dataloader, loss_fn, accuracy_fn, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82eac6b-91fe-4cae-822f-e5af251b7a23",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "final_results = pd.DataFrame([\n",
    "    model_0_results,\n",
    "    model_1_results,\n",
    "    model_2_results])\n",
    "\n",
    "final_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8d5c90-b549-4917-8c63-13bdd0648735",
   "metadata": {},
   "source": [
    "final_results.set_index(\"model_name\")[\"model_accuracy\"].plot(kind=\"barh\")\n",
    "plt.xlabel(\"accuracy (%)\")\n",
    "plt.ylabel(\"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7e9690-2ea2-4200-8d51-ddb989ea84c2",
   "metadata": {},
   "source": [
    "# return numpy array\n",
    "def model_do_predictions(model: torch.nn.Module,\n",
    "                data_loader: torch.utils.data.DataLoader) -> np.ndarray:\n",
    "    predictions = np.array([])\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for batch, (X, _) in enumerate(data_loader):\n",
    "            \n",
    "            pred = model(X)\n",
    "            predictions = np.append(predictions, pred.detach().numpy().argmax(axis=1))  \n",
    "        \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca61a489-cfe4-4521-b245-3f8d16785322",
   "metadata": {},
   "source": [
    "test_predictions = model_do_predictions(model_2, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e7c3cb-042f-4f9a-a9e1-75b30d79d029",
   "metadata": {},
   "source": [
    "from torchmetrics import ConfusionMatrix\n",
    "from mlxtend.plotting import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2c3ba3-c315-44c4-98ee-7bf7be455bb1",
   "metadata": {},
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "\n",
    "conf_mat = confusion_matrix(test_predictions, test_data.targets.detach().numpy())\n",
    "\n",
    "plt.imshow(conf_mat)\n",
    "\n",
    "df_cm = pd.DataFrame(conf_mat, index=class_names, columns=class_names)\n",
    "\n",
    "sn.heatmap(df_cm, annot=True, fmt='g', annot_kws={\"size\": 10}, cmap='Blues')\n",
    "\n",
    "df_cm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
