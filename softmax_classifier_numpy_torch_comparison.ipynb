{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f17be233-cbc1-48a0-b249-7d34572e66a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48088d79-ba48-4cee-9197-4a7b469c17ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.62313406  8.87720831  4.84001326]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((750, 3), (750,), (250, 3), (250,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = 1000\n",
    "\n",
    "X, y = make_blobs(n_samples=m,\n",
    "                  n_features=3,\n",
    "                  shuffle=True,\n",
    "                  random_state=42)\n",
    "\n",
    "train_size = 750\n",
    "\n",
    "X_train, y_train = X[:train_size], y[:train_size]\n",
    "X_test, y_test = X[train_size:], y[train_size:]\n",
    "\n",
    "print(X_train[0])\n",
    "\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7fb4f28-cd32-4ba4-ae91-66116055acda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits:   [-1.7134843 -5.1125574  6.2606854]\n",
      "Softmax:  [3.44118191e-04 1.14949935e-05 9.99644399e-01]\n",
      "Loss:     0.00035565728\n",
      "\n",
      "\n",
      "Logits:\n",
      " tensor([[ 2.4593, -0.9181,  6.0376],\n",
      "        [-1.2193,  1.2717, -5.5731],\n",
      "        [ 1.9715, -0.9329,  6.3642],\n",
      "        [-0.6470, -4.9998,  7.2019],\n",
      "        [ 0.9929, -2.1822,  7.1968]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "Softmax torch.Size([5, 3]):\n",
      " tensor([[2.7139e-02, 9.2640e-04, 9.7193e-01],\n",
      "        [7.6420e-02, 9.2260e-01, 9.8257e-04],\n",
      "        [1.2207e-02, 6.6876e-04, 9.8712e-01],\n",
      "        [3.9003e-04, 5.0199e-06, 9.9961e-01],\n",
      "        [2.0172e-03, 8.4300e-05, 9.9790e-01]], grad_fn=<SoftmaxBackward0>)\n",
      "\n",
      "Loss:\n",
      " tensor(2.8599, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "\n",
      "tensor([[3.4412e-04, 1.1495e-05, 9.9964e-01]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor(2.8599, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def printt(x): print(['{:f}'.format(i) for i in x.detach().numpy()])\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "layers = nn.Sequential(\n",
    "    nn.Linear(in_features=3, out_features=3))\n",
    "    #nn.Softmax(dim=0))\n",
    "\n",
    "\n",
    "    \n",
    "X_train_tensor = torch.from_numpy(X_train).type(torch.float32)\n",
    "y_train_tensor = torch.Tensor(y_train).type(torch.long)\n",
    "\n",
    "X_train_tensor.requires_grad = True\n",
    "\n",
    "\n",
    "\n",
    "# single\n",
    "\n",
    "i = 5\n",
    "\n",
    "samples = X_train_tensor[i].unsqueeze(0)\n",
    "preds = layers(samples)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "loss = loss_fn(preds, y_train_tensor[i].unsqueeze(0))\n",
    "\n",
    "print(\"Logits:  \", preds.detach().numpy().squeeze())\n",
    "print(\"Softmax: \", torch.softmax(preds, dim=1).detach().numpy().squeeze())\n",
    "print(\"Loss:    \", loss.detach().numpy())\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "# multi\n",
    "\n",
    "i = 5\n",
    "\n",
    "samples = X_train_tensor[:i]\n",
    "labels = y_train_tensor[:i]\n",
    "logits = layers(samples)\n",
    "loss = loss_fn(logits, labels)\n",
    "\n",
    "print(\"Logits:\\n\", logits)\n",
    "print(f\"\\nSoftmax {logits.shape}:\\n\", torch.softmax(logits, dim=1))\n",
    "print(\"\\nLoss:\\n\", loss)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(nn.functional.softmax(preds, dim=1))\n",
    "print(loss)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "loss.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05267b2c-7e82-4b4c-8b9e-083c0a929492",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb46b6da-88c8-46fe-b6c1-5e585be2ad8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a15b44-4976-4ddf-b167-2028c4a99dc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
