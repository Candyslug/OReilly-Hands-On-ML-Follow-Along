{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aaa4c896-30ad-451c-b865-b179d544b47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e3af0629-e954-4041-b550-08fff03a12c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((750, 3), (750,), (250, 3), (250,))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = 1000\n",
    "\n",
    "X, y = make_blobs(n_samples=m,\n",
    "                  n_features=3,\n",
    "                  shuffle=True,\n",
    "                  random_state=42)\n",
    "\n",
    "train_size = 750\n",
    "\n",
    "X_train, y_train = X[:train_size], y[:train_size]\n",
    "X_test, y_test = X[train_size:], y[train_size:]\n",
    "\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "id": "4a9df921-c48c-48e2-98b9-6319a65c4e8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.62313406,  8.87720831,  4.84001326],\n",
       "       [ 1.71267112, -5.15279335, -6.78367587],\n",
       "       [-1.94449656,  8.37968109,  5.63563107],\n",
       "       [-8.99181204,  8.71078033,  3.16892415],\n",
       "       [-4.30124272,  8.56751262,  5.84480176],\n",
       "       [-9.5004586 ,  6.72571453,  2.36111484],\n",
       "       [-3.43328046,  6.40173712,  5.59024852],\n",
       "       [-9.15841974,  6.92686076,  2.50482428],\n",
       "       [-2.84344471,  8.69643922,  4.95764587],\n",
       "       [-7.03616295,  7.70662313,  2.50260489]])"
      ]
     },
     "execution_count": 561,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "89191536-9c6f-4310-a331-78cf0db077e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# matching torch model test\n",
    "\n",
    "bias = np.array([-0.4236,  0.5018,  0.1081])\n",
    "\n",
    "weights = np.array(\n",
    "        [[ 0.4414,  0.4792, -0.1353],\n",
    "        [ 0.5304, -0.1265,  0.1165],\n",
    "        [-0.2811,  0.3391,  0.5090]]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd912579-9490-4629-b7f4-7320d7efffc2",
   "metadata": {},
   "source": [
    "<img src=\"https://www.gstatic.com/education/formulas2/472522532/en/softmax_function.svg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "5e81c80d-f670-4c0a-968c-f531bf707ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.62313406  8.87720831  4.84001326]\n",
      " [ 1.71267112 -5.15279335 -6.78367587]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 1, 3)"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = X_train[:2]\n",
    "print(sample)\n",
    "sample[:,None].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "id": "8a99d71e-7a49-414b-bed5-6a6f18228675",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits:   [-1.71359886 -5.11297625  6.26117616]\n",
      "Softmax:  [3.43910028e-04 1.14845504e-05 9.99644605e-01]\n",
      "Loss:     0.0003554577461493537\n",
      "\n",
      "\n",
      "Logits: (5, 3)\n",
      " [[ 2.45905305 -0.91821561  6.03819107]\n",
      " [-1.21901419  1.27173088 -5.5735351 ]\n",
      " [ 1.97114151 -0.93303961  6.36478406]\n",
      " [-0.64713534 -5.00019115  7.20250637]\n",
      " [ 0.99258183 -2.18245008  7.19742695]]\n",
      "\n",
      "Softmax: (5, 3)\n",
      " [[2.71173407e-02 9.25801654e-04 9.71956858e-01]\n",
      " [7.64343987e-02 9.22583527e-01 9.82074543e-04]\n",
      " [1.21966860e-02 6.68300838e-04 9.87135013e-01]\n",
      " [3.89737730e-04 5.01492372e-06 9.99605247e-01]\n",
      " [2.01538116e-03 8.42283621e-05 9.97900390e-01]]\n",
      "\n",
      "Loss:\n",
      " 2.860418402503541\n",
      "\n",
      "\n",
      "Average loss: 0.5720836805007081\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (5,3) (2,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [589], line 73\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m/\u001b[39mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 73\u001b[0m d1, d2 \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_loss_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msoftmax_multi\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m d1\u001b[38;5;241m.\u001b[39mshape\n",
      "Cell \u001b[0;32mIn [589], line 35\u001b[0m, in \u001b[0;36mcompute_loss_gradients\u001b[0;34m(X, y_pred, y_true)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_loss_gradients\u001b[39m(X, y_pred, y_true):\n\u001b[0;32m---> 35\u001b[0m     dw \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43my_true\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\n\u001b[1;32m     36\u001b[0m     db \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39mm) \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msum(y_pred\u001b[38;5;241m-\u001b[39my_true)\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dw, db\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (5,3) (2,) "
     ]
    }
   ],
   "source": [
    "def softmax(X):\n",
    "    a = np.exp(X)\n",
    "    return a/np.sum(a)\n",
    "\n",
    "def forward(X):\n",
    "    pred = weights.dot(X) + bias\n",
    "    #pred = softmax(pred)\n",
    "    return pred\n",
    "\n",
    "def softmax_multi(X):\n",
    "    expd = np.exp(X)\n",
    "    sums = np.sum(expd, axis=1)\n",
    "    return (expd.T/sums).T\n",
    "    \n",
    "def forward_multi(X):\n",
    "    pred = np.sum(weights*X[:,None], axis=2) + bias\n",
    "    #red = softmax_multi(pred)\n",
    "    return pred\n",
    "\n",
    "def one_hot_batch(a):\n",
    "    return np.squeeze(np.eye(3)[a.reshape(-1)])\n",
    "\n",
    "def cross_entropy_loss(y_pred, y_true):\n",
    "    y_pred = softmax(y_pred)\n",
    "    lce = -np.sum(y_true*(np.log(y_pred)))\n",
    "    return lce\n",
    "\n",
    "def cross_entropy_loss_multi(y_pred, y_true):\n",
    "    y_pred = softmax_multi(y_pred)\n",
    "    #lce = -np.sum(y_true*np.log(y_pred + 1e-7)) # recommended by ChatGPT\n",
    "    lce = -np.sum(y_true*np.log(y_pred))\n",
    "    return lce/(len(y_true))\n",
    "\n",
    "def compute_loss_gradients(X, y_pred, y_true):\n",
    "    dw = (y_pred-y_true) / X.shape\n",
    "    db = (1/m) * np.sum(y_pred-y_true)\n",
    "    return dw, db\n",
    "\n",
    "\n",
    "# single\n",
    "\n",
    "i = 5\n",
    "\n",
    "sample = X_train[i]\n",
    "label = one_hot_batch(y_train[i])\n",
    "logits = forward(sample)\n",
    "loss = cross_entropy_loss(logits, label)\n",
    "\n",
    "print(\"Logits:  \", logits)\n",
    "print(\"Softmax: \", softmax(logits))\n",
    "print(\"Loss:    \", loss)\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# multi\n",
    "\n",
    "i = 5\n",
    "\n",
    "sample = X_train[:i]\n",
    "labels = one_hot_batch(y_train[:i])\n",
    "logits = forward_multi(sample)\n",
    "loss = cross_entropy_loss_multi(logits, labels)\n",
    "\n",
    "\n",
    "print(f\"Logits: {logits.shape}\\n\", logits)\n",
    "print(f\"\\nSoftmax: {softmax(logits).shape}\\n\", softmax_multi(logits))\n",
    "print(\"\\nLoss:\\n\", loss)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(f\"Average loss: {loss/i}\\n\")\n",
    "\n",
    "d1, d2 = compute_loss_gradients(sample, softmax_multi(logits), labels)\n",
    "\n",
    "d1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9d853f-a6d6-4fa7-a8fa-49f42d0a1619",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
